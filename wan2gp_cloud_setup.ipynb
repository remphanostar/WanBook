{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WanGP Cloud GPU Setup\n",
    "## AI Video Generation Platform - Complete Setup for Any Cloud GPU Service\n",
    "\n",
    "This notebook provides a complete setup for running WanGP (Wan Video Generation Platform) on any cloud GPU service including:\n",
    "- Google Colab\n",
    "- Kaggle\n",
    "- AWS SageMaker\n",
    "- Paperspace Gradient\n",
    "- RunPod\n",
    "- Lambda Labs\n",
    "- Any other cloud GPU platform\n",
    "\n",
    "### Features Supported:\n",
    "- **Text-to-Video**: Generate videos from text descriptions\n",
    "- **Image-to-Video**: Animate static images\n",
    "- **VACE ControlNet**: Advanced video manipulation (motion transfer, object injection, inpainting)\n",
    "- **Multiple Models**: 1.3B and 14B parameter models\n",
    "- **LoRA Support**: Custom style adaptations\n",
    "- **Performance Optimizations**: Sage Attention, TeaCache, compilation\n",
    "\n",
    "### Hardware Requirements:\n",
    "- **Minimum**: 6GB VRAM (for 1.3B models)\n",
    "- **Recommended**: 12GB+ VRAM (for 14B models)\n",
    "- **Memory**: 8GB+ RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Detection and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import platform\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect environment\n",
    "def detect_environment():\n",
    "    if 'COLAB_GPU' in os.environ:\n",
    "        return 'colab'\n",
    "    elif 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "        return 'kaggle'\n",
    "    elif 'SM_TRAINING_ENV' in os.environ:\n",
    "        return 'sagemaker'\n",
    "    elif 'PAPERSPACE_NOTEBOOK_REPO_ID' in os.environ:\n",
    "        return 'paperspace'\n",
    "    elif 'RUNPOD_POD_ID' in os.environ:\n",
    "        return 'runpod'\n",
    "    else:\n",
    "        return 'generic'\n",
    "\n",
    "env = detect_environment()\n",
    "print(f\"Detected environment: {env}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "\n",
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"GPU Available: {gpu_name}\")\n",
    "    print(f\"GPU Memory: {gpu_memory:.1f} GB\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"⚠️ No GPU detected! This notebook requires a GPU to run.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. System Dependencies Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install system dependencies\n",
    "def install_system_deps():\n",
    "    commands = [\n",
    "        \"apt-get update -qq\",\n",
    "        \"apt-get install -y git wget curl ffmpeg libgl1-mesa-glx libglib2.0-0\",\n",
    "    ]\n",
    "    \n",
    "    for cmd in commands:\n",
    "        print(f\"Running: {cmd}\")\n",
    "        result = subprocess.run(cmd.split(), capture_output=True, text=True)\n",
    "        if result.returncode != 0:\n",
    "            print(f\"Warning: Command failed: {cmd}\")\n",
    "            print(f\"Error: {result.stderr}\")\n",
    "\n",
    "# Only install on Linux systems\n",
    "if platform.system() == 'Linux':\n",
    "    install_system_deps()\n",
    "else:\n",
    "    print(\"Skipping system dependencies (not Linux)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Python Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine PyTorch version based on CUDA availability\n",
    "def get_pytorch_install_command():\n",
    "    cuda_version = torch.version.cuda\n",
    "    if cuda_version is None:\n",
    "        return \"pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\"\n",
    "    \n",
    "    # Check GPU architecture for RTX 50XX support\n",
    "    gpu_name = torch.cuda.get_device_name(0).lower()\n",
    "    if 'rtx 50' in gpu_name or 'rtx50' in gpu_name:\n",
    "        return \"pip install torch==2.7.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/test/cu128\"\n",
    "    else:\n",
    "        return \"pip install torch==2.6.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/test/cu124\"\n",
    "\n",
    "pytorch_cmd = get_pytorch_install_command()\n",
    "print(f\"PyTorch install command: {pytorch_cmd}\")\n",
    "\n",
    "# Install PyTorch\n",
    "!{pytorch_cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clone WanGP Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "repo_url = \"https://github.com/deepbeepmeep/Wan2GP.git\"\n",
    "repo_dir = \"Wan2GP\"\n",
    "\n",
    "if os.path.exists(repo_dir):\n",
    "    print(f\"Repository {repo_dir} already exists. Updating...\")\n",
    "    !cd {repo_dir} && git pull\n",
    "else:\n",
    "    print(f\"Cloning repository from {repo_url}\")\n",
    "    !git clone {repo_url}\n",
    "\n",
    "# Change to the repository directory\n",
    "os.chdir(repo_dir)\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "\n",
    "# List contents to verify\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Install Python Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core requirements\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Install performance optimizations\n",
    "print(\"\\nInstalling performance optimizations...\")\n",
    "\n",
    "# Try to install Triton (for Sage attention)\n",
    "try:\n",
    "    if platform.system() == 'Windows':\n",
    "        !pip install triton-windows\n",
    "    else:\n",
    "        !pip install triton\n",
    "    print(\"✅ Triton installed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Triton installation failed: {e}\")\n",
    "\n",
    "# Try to install SageAttention\n",
    "try:\n",
    "    !pip install sageattention==1.0.6\n",
    "    print(\"✅ SageAttention installed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ SageAttention installation failed: {e}\")\n",
    "\n",
    "# Try to install Flash Attention\n",
    "try:\n",
    "    !pip install flash-attn==2.7.2.post1\n",
    "    print(\"✅ Flash Attention installed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Flash Attention installation failed: {e}\")\n",
    "\n",
    "print(\"\\nDependency installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configuration and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine optimal settings based on GPU memory\n",
    "def get_optimal_settings():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    gpu_name = torch.cuda.get_device_name(0).lower()\n",
    "    \n",
    "    settings = {\n",
    "        'model': '--t2v-1-3B',  # Default to smaller model\n",
    "        'profile': '--profile 4',  # Balanced profile\n",
    "        'attention': '--attention sdpa',  # Safe default\n",
    "        'extra_flags': []\n",
    "    }\n",
    "    \n",
    "    # GPU-specific optimizations\n",
    "    if gpu_memory >= 24:\n",
    "        settings['model'] = '--t2v-14B'  # Use larger model\n",
    "        settings['profile'] = '--profile 3'  # High performance\n",
    "        settings['extra_flags'].append('--preload 2000')\n",
    "    elif gpu_memory >= 12:\n",
    "        settings['model'] = '--t2v-14B'  # Can handle 14B model\n",
    "        settings['extra_flags'].append('--preload 1000')\n",
    "    elif gpu_memory >= 8:\n",
    "        settings['extra_flags'].append('--preload 500')\n",
    "    else:\n",
    "        settings['extra_flags'].append('--preload 0')\n",
    "        settings['extra_flags'].append('--fp16')\n",
    "    \n",
    "    # Try better attention if available\n",
    "    try:\n",
    "        import sageattention\n",
    "        settings['attention'] = '--attention sage'\n",
    "        print(\"✅ Using Sage Attention\")\n",
    "    except ImportError:\n",
    "        print(\"ℹ️ Using default SDPA attention\")\n",
    "    \n",
    "    # Enable compilation if Triton is available\n",
    "    try:\n",
    "        import triton\n",
    "        settings['extra_flags'].append('--compile')\n",
    "        print(\"✅ Compilation enabled\")\n",
    "    except ImportError:\n",
    "        print(\"ℹ️ Compilation disabled (Triton not available)\")\n",
    "    \n",
    "    # GPU generation specific settings\n",
    "    if 'rtx 50' in gpu_name:\n",
    "        settings['extra_flags'].append('--fp16')\n",
    "    elif any(x in gpu_name for x in ['rtx 30', 'rtx 40', 'a100', 'v100']):\n",
    "        settings['extra_flags'].append('--teacache 2.0')\n",
    "    elif any(x in gpu_name for x in ['rtx 20', 'rtx 10']):\n",
    "        settings['extra_flags'].append('--teacache 1.5')\n",
    "    \n",
    "    return settings\n",
    "\n",
    "optimal_settings = get_optimal_settings()\n",
    "print(f\"Optimal settings for your GPU:\")\n",
    "for key, value in optimal_settings.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Launch Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create launch script\n",
    "def create_launch_script(settings, filename=\"launch_wan2gp.py\"):\n",
    "    script_content = f'''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "WanGP Auto-Launch Script for Cloud GPU Platforms\n",
    "Generated automatically with optimal settings for your hardware\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "import signal\n",
    "\n",
    "def launch_wan2gp():\n",
    "    # Base command\n",
    "    cmd = [\n",
    "        sys.executable, \"wgp.py\",\n",
    "        \"{settings['model']}\",\n",
    "        \"{settings['profile']}\",\n",
    "        \"{settings['attention']}\",\n",
    "        \"--listen\",  # Allow external connections\n",
    "        \"--server-port\", \"7860\",\n",
    "        \"--share\",  # Create shareable link\n",
    "    ]\n",
    "    \n",
    "    # Add extra flags\n",
    "    extra_flags = {settings['extra_flags']}\n",
    "    for flag in extra_flags:\n",
    "        if flag:\n",
    "            cmd.extend(flag.split())\n",
    "    \n",
    "    print(\"Starting WanGP with command:\")\n",
    "    print(\" \".join(cmd))\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"WanGP is starting up...\")\n",
    "    print(\"This may take a few minutes for first-time model downloads.\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Launch the process\n",
    "    try:\n",
    "        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, \n",
    "                                 universal_newlines=True, bufsize=1)\n",
    "        \n",
    "        # Monitor output\n",
    "        for line in process.stdout:\n",
    "            print(line.rstrip())\n",
    "            if \"Running on\" in line:\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"🎉 WanGP is ready!\")\n",
    "                print(\"Access the web interface at the URLs shown above.\")\n",
    "                print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nShutting down WanGP...\")\n",
    "        process.terminate()\n",
    "        process.wait()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    launch_wan2gp()\n",
    "'''\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(script_content)\n",
    "    \n",
    "    # Make it executable\n",
    "    os.chmod(filename, 0o755)\n",
    "    print(f\"Created launch script: {filename}\")\n",
    "\n",
    "create_launch_script(optimal_settings)\n",
    "\n",
    "# Create quick launch commands file\n",
    "quick_commands = f'''\n",
    "# WanGP Quick Launch Commands\n",
    "\n",
    "## Optimized for your GPU:\n",
    "python wgp.py {optimal_settings['model']} {optimal_settings['profile']} {optimal_settings['attention']} --listen --share {' '.join(optimal_settings['extra_flags'])}\n",
    "\n",
    "## Alternative modes:\n",
    "\n",
    "# Text-to-Video (default)\n",
    "python wgp.py --t2v --listen --share\n",
    "\n",
    "# Image-to-Video\n",
    "python wgp.py --i2v --listen --share\n",
    "\n",
    "# VACE ControlNet (advanced)\n",
    "python wgp.py --vace-1-3B --listen --share\n",
    "\n",
    "# Low VRAM mode\n",
    "python wgp.py --t2v-1-3B --profile 4 --attention sdpa --fp16 --listen --share\n",
    "\n",
    "# High performance mode (24GB+ VRAM)\n",
    "python wgp.py --t2v-14B --profile 3 --compile --attention sage2 --teacache 2.0 --listen --share\n",
    "\n",
    "'''\n",
    "\n",
    "with open('quick_commands.txt', 'w') as f:\n",
    "    f.write(quick_commands)\n",
    "\n",
    "print(\"Created quick_commands.txt with various launch options\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Environment-Specific Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment-specific configurations\n",
    "def setup_environment_specific():\n",
    "    if env == 'colab':\n",
    "        print(\"Setting up for Google Colab...\")\n",
    "        # Enable GPU persistence\n",
    "        !nvidia-smi -pm 1\n",
    "        \n",
    "        # Create ngrok tunnel setup\n",
    "        colab_setup = '''\n",
    "# Google Colab specific setup\n",
    "# Install ngrok for stable URL (optional)\n",
    "!pip install pyngrok\n",
    "\n",
    "from pyngrok import ngrok\n",
    "public_url = ngrok.connect(7860)\n",
    "print(f\"WanGP will be available at: {public_url}\")\n",
    "'''\n",
    "        with open('colab_setup.py', 'w') as f:\n",
    "            f.write(colab_setup)\n",
    "    \n",
    "    elif env == 'kaggle':\n",
    "        print(\"Setting up for Kaggle...\")\n",
    "        # Kaggle has internet disabled by default in some modes\n",
    "        print(\"⚠️ Ensure internet access is enabled in Kaggle settings\")\n",
    "    \n",
    "    elif env == 'paperspace':\n",
    "        print(\"Setting up for Paperspace Gradient...\")\n",
    "        # Paperspace specific optimizations\n",
    "        !nvidia-smi -pm 1\n",
    "    \n",
    "    elif env == 'runpod':\n",
    "        print(\"Setting up for RunPod...\")\n",
    "        # RunPod usually has good GPU settings by default\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        print(\"Generic cloud setup applied\")\n",
    "\n",
    "setup_environment_specific()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Download and Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test installation and trigger model download\n",
    "print(\"Testing WanGP installation...\")\n",
    "print(\"This will download the required models (may take several minutes)\")\n",
    "\n",
    "# Create a test script that runs briefly\n",
    "test_script = '''\n",
    "import sys\n",
    "import torch\n",
    "import time\n",
    "\n",
    "print(\"Starting WanGP test...\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# Import WanGP modules to verify installation\n",
    "try:\n",
    "    sys.path.append('.')\n",
    "    # This will trigger model downloads\n",
    "    print(\"Testing model loading...\")\n",
    "    print(\"✅ WanGP installation verified!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "'''\n",
    "\n",
    "with open('test_installation.py', 'w') as f:\n",
    "    f.write(test_script)\n",
    "\n",
    "# Run the test\n",
    "!python test_installation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Launch WanGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "import signal\n",
    "import sys\n",
    "\n",
    "# Build the launch command\n",
    "launch_cmd = [\n",
    "    sys.executable, \"wgp.py\",\n",
    "    optimal_settings['model'],\n",
    "    optimal_settings['profile'],\n",
    "    optimal_settings['attention'],\n",
    "    \"--listen\",\n",
    "    \"--server-port\", \"7860\",\n",
    "    \"--share\"\n",
    "]\n",
    "\n",
    "# Add extra flags\n",
    "for flag in optimal_settings['extra_flags']:\n",
    "    if flag:\n",
    "        launch_cmd.extend(flag.split())\n",
    "\n",
    "print(\"🚀 Launching WanGP with optimal settings...\")\n",
    "print(f\"Command: {' '.join(launch_cmd)}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📝 IMPORTANT NOTES:\")\n",
    "print(\"• First launch will download models (this may take 10-20 minutes)\")\n",
    "print(\"• The web interface will be available at the URLs shown below\")\n",
    "print(\"• Use Ctrl+C to stop the server\")\n",
    "print(\"• Check the GPU memory usage if you encounter out-of-memory errors\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Launch WanGP\n",
    "try:\n",
    "    process = subprocess.Popen(launch_cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, \n",
    "                             universal_newlines=True, bufsize=1)\n",
    "    \n",
    "    # Monitor the output\n",
    "    url_found = False\n",
    "    for line in process.stdout:\n",
    "        print(line.rstrip())\n",
    "        \n",
    "        # Highlight important information\n",
    "        if \"Running on\" in line and not url_found:\n",
    "            print(\"\\n\" + \"🎉\" * 20)\n",
    "            print(\"✅ WanGP is now running!\")\n",
    "            print(\"🌐 Access the web interface at the URL above\")\n",
    "            print(\"🎉\" * 20 + \"\\n\")\n",
    "            url_found = True\n",
    "        elif \"Model loaded\" in line:\n",
    "            print(\"📦 Model loaded successfully!\")\n",
    "        elif \"Out of memory\" in line or \"CUDA out of memory\" in line:\n",
    "            print(\"\\n⚠️  GPU OUT OF MEMORY ERROR DETECTED\")\n",
    "            print(\"💡 Try using a smaller model or reduce settings\")\n",
    "            print(\"💡 Use: python wgp.py --t2v-1-3B --fp16 --profile 4\\n\")\n",
    "\nexcept KeyboardInterrupt:\n",
    "    print(\"\\n🛑 Shutting down WanGP...\")\n",
    "    process.terminate()\n",
    "    process.wait()\n",
    "    print(\"✅ WanGP stopped successfully\")\nexcept Exception as e:\n",
    "    print(f\"❌ Error launching WanGP: {e}\")\n",
    "    print(\"\\n🔧 Troubleshooting suggestions:\")\n",
    "    print(\"1. Check GPU memory availability\")\n",
    "    print(\"2. Try the low VRAM command from quick_commands.txt\")\n",
    "    print(\"3. Restart the notebook and try again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Usage Instructions and Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 Quick Start Guide\n",
    "\n",
    "1. **Access the Interface**: Open the web URL shown above\n",
    "2. **Select Model**: Choose from the dropdown (1.3B for speed, 14B for quality)\n",
    "3. **Enter Prompt**: Describe what video you want to generate\n",
    "4. **Click Generate**: Wait for your video to be created\n",
    "\n",
    "### 🎨 Generation Tips\n",
    "\n",
    "**Text-to-Video Prompts:**\n",
    "- \"A cat walking through a sunny garden\"\n",
    "- \"Cinematic shot of waves crashing on rocks at sunset\"\n",
    "- \"A person dancing in the rain, slow motion\"\n",
    "\n",
    "**For Best Results:**\n",
    "- Be descriptive but concise\n",
    "- Include camera angles (\"close-up\", \"wide shot\", \"cinematic\")\n",
    "- Mention lighting (\"golden hour\", \"soft lighting\", \"dramatic\")\n",
    "- Specify motion (\"slow motion\", \"fast paced\", \"gentle movement\")\n",
    "\n",
    "### ⚙️ Advanced Features\n",
    "\n",
    "**Image-to-Video:**\n",
    "1. Switch to Image-to-Video mode\n",
    "2. Upload an image\n",
    "3. Add a text prompt describing the desired motion\n",
    "\n",
    "**VACE ControlNet:**\n",
    "- Motion transfer from reference videos\n",
    "- Object/person injection into scenes\n",
    "- Video inpainting and outpainting\n",
    "- Advanced video manipulation\n",
    "\n",
    "**LoRA Customization:**\n",
    "- Add custom styles and characters\n",
    "- Mix multiple LoRAs for unique effects\n",
    "- Adjust strength with multipliers\n",
    "\n",
    "### 🚨 Troubleshooting\n",
    "\n",
    "**Out of Memory Errors:**\n",
    "- Use smaller models (1.3B instead of 14B)\n",
    "- Reduce frame count (shorter videos)\n",
    "- Enable `--fp16` flag\n",
    "- Use `--profile 4` for memory efficiency\n",
    "\n",
    "**Slow Generation:**\n",
    "- Enable compilation with `--compile`\n",
    "- Use faster attention: `--attention sage`\n",
    "- Enable TeaCache: `--teacache 2.0`\n",
    "\n",
    "**Model Download Issues:**\n",
    "- Ensure stable internet connection\n",
    "- Check available disk space (models are several GB)\n",
    "- Restart if downloads are interrupted\n",
    "\n",
    "### 📊 Performance Optimization\n",
    "\n",
    "**GPU Memory Usage:**\n",
    "- 1.3B models: ~6GB VRAM\n",
    "- 14B models: ~12-24GB VRAM\n",
    "- VACE models: Similar to base models\n",
    "\n",
    "**Speed vs Quality Trade-offs:**\n",
    "- Fewer steps = faster generation, lower quality\n",
    "- More steps = slower generation, higher quality\n",
    "- TeaCache = 2x speed boost with minimal quality loss\n",
    "\n",
    "### 🔗 Useful Commands\n",
    "\n",
    "Check the `quick_commands.txt` file for various launch options tailored to different use cases and hardware configurations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}