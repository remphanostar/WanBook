{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exHHIjZUkaBo"
      },
      "source": [
        "# WanGP Cloud GPU Setup\n",
        "## AI Video Generation Platform - Complete Setup for Any Cloud GPU Service\n",
        "\n",
        "This notebook provides a complete setup for running WanGP (Wan Video Generation Platform) on any cloud GPU service including:\n",
        "- Google Colab\n",
        "- Kaggle\n",
        "- AWS SageMaker\n",
        "- Paperspace Gradient\n",
        "- RunPod\n",
        "- Lambda Labs\n",
        "- Any other cloud GPU platform\n",
        "\n",
        "### Features Supported:\n",
        "- **Text-to-Video**: Generate videos from text descriptions\n",
        "- **Image-to-Video**: Animate static images\n",
        "- **VACE ControlNet**: Advanced video manipulation (motion transfer, object injection, inpainting)\n",
        "- **Multiple Models**: 1.3B and 14B parameter models\n",
        "- **LoRA Support**: Custom style adaptations\n",
        "- **Performance Optimizations**: Sage Attention, TeaCache, compilation\n",
        "\n",
        "### Hardware Requirements:\n",
        "- **Minimum**: 6GB VRAM (for 1.3B models)\n",
        "- **Recommended**: 12GB+ VRAM (for 14B models)\n",
        "- **Memory**: 8GB+ RAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KvOWa7_kaBp"
      },
      "source": [
        "## 1. Environment Detection and Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# New cell to setup environment variables\n",
        "import os\n",
        "\n",
        "# Set XDG_RUNTIME_DIR to prevent errors\n",
        "if 'XDG_RUNTIME_DIR' not in os.environ:\n",
        "    # Determine user for unique directory\n",
        "    import getpass\n",
        "    runtime_dir = f\"/tmp/runtime-{getpass.getuser()}\"\n",
        "    os.environ['XDG_RUNTIME_DIR'] = runtime_dir\n",
        "    if not os.path.exists(runtime_dir):\n",
        "        os.makedirs(runtime_dir, mode=0o700)\n",
        "    print(f\"✅ Set XDG_RUNTIME_DIR to {runtime_dir}\")\n",
        "else:\n",
        "    print(f\"✓ XDG_RUNTIME_DIR already set to {os.environ['XDG_RUNTIME_DIR']}\")\n",
        "\n",
        "# Fix ALSA warnings by setting dummy audio drivers\n",
        "os.environ['SDL_AUDIODRIVER'] = 'dummy'\n",
        "os.environ['ALSA_CONFIG_PATH'] = '/dev/null'\n",
        "print(\"✅ Configured audio to use dummy drivers\")\n",
        "\n",
        "print(\"Environment setup complete - WanGP should now launch without environment errors\")"
      ],
      "metadata": {
        "id": "lHjKTstqzeWd",
        "outputId": "c931808a-8800-47dd-d8e5-001be9cb7661",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Set XDG_RUNTIME_DIR to /tmp/runtime-root\n",
            "✅ Configured audio to use dummy drivers\n",
            "Environment setup complete - WanGP should now launch without environment errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VPZmtVWbkaBq",
        "outputId": "20b95c7a-f138-4647-fe83-dff7d246e65b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected environment: colab\n",
            "Python version: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
            "Platform: Linux-6.1.123+-x86_64-with-glibc2.35\n",
            "GPU Available: Tesla T4\n",
            "GPU Memory: 14.7 GB\n",
            "CUDA Version: 12.4\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import platform\n",
        "import torch\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Detect environment\n",
        "def detect_environment():\n",
        "    if 'COLAB_GPU' in os.environ:\n",
        "        return 'colab'\n",
        "    elif 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
        "        return 'kaggle'\n",
        "    elif 'SM_TRAINING_ENV' in os.environ:\n",
        "        return 'sagemaker'\n",
        "    elif 'PAPERSPACE_NOTEBOOK_REPO_ID' in os.environ:\n",
        "        return 'paperspace'\n",
        "    elif 'RUNPOD_POD_ID' in os.environ:\n",
        "        return 'runpod'\n",
        "    else:\n",
        "        return 'generic'\n",
        "\n",
        "env = detect_environment()\n",
        "print(f\"Detected environment: {env}\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"Platform: {platform.platform()}\")\n",
        "\n",
        "# Check GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    gpu_count = torch.cuda.device_count()\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "    print(f\"GPU Available: {gpu_name}\")\n",
        "    print(f\"GPU Memory: {gpu_memory:.1f} GB\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "else:\n",
        "    print(\"⚠️ No GPU detected! This notebook requires a GPU to run.\")\n",
        "    sys.exit(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NeSg8XjkaBr"
      },
      "source": [
        "## 2. System Dependencies Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TN4Xck7RkaBr",
        "outputId": "81cfe027-feca-4739-f08e-4b31f9a681ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running: apt-get update -qq\n",
            "Running: apt-get install -y git wget curl ffmpeg libgl1-mesa-glx libglib2.0-0\n"
          ]
        }
      ],
      "source": [
        "# Install system dependencies\n",
        "def install_system_deps():\n",
        "    commands = [\n",
        "        \"apt-get update -qq\",\n",
        "        \"apt-get install -y git wget curl ffmpeg libgl1-mesa-glx libglib2.0-0\",\n",
        "    ]\n",
        "\n",
        "    for cmd in commands:\n",
        "        print(f\"Running: {cmd}\")\n",
        "        result = subprocess.run(cmd.split(), capture_output=True, text=True)\n",
        "        if result.returncode != 0:\n",
        "            print(f\"Warning: Command failed: {cmd}\")\n",
        "            print(f\"Error: {result.stderr}\")\n",
        "\n",
        "# Only install on Linux systems\n",
        "if platform.system() == 'Linux':\n",
        "    install_system_deps()\n",
        "else:\n",
        "    print(\"Skipping system dependencies (not Linux)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn1q8PCckaBr"
      },
      "source": [
        "## 3. Python Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EJfmS7Y9kaBs",
        "outputId": "1d72adbe-1094-43b0-879d-662132d9b50f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch install command: pip install torch==2.6.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/test/cu124\n",
            "Looking in indexes: https://download.pytorch.org/whl/test/cu124\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/test/cu124/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/test/cu124/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m981.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/test/cu124/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/test/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/test/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/test/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/test/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/test/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/test/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/test/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0) (3.0.2)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "# Determine PyTorch version based on CUDA availability\n",
        "def get_pytorch_install_command():\n",
        "    cuda_version = torch.version.cuda\n",
        "    if cuda_version is None:\n",
        "        return \"pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\"\n",
        "\n",
        "    # Check GPU architecture for RTX 50XX support\n",
        "    gpu_name = torch.cuda.get_device_name(0).lower()\n",
        "    if 'rtx 50' in gpu_name or 'rtx50' in gpu_name:\n",
        "        return \"pip install torch==2.7.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/test/cu128\"\n",
        "    else:\n",
        "        return \"pip install torch==2.6.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/test/cu124\"\n",
        "\n",
        "pytorch_cmd = get_pytorch_install_command()\n",
        "print(f\"PyTorch install command: {pytorch_cmd}\")\n",
        "\n",
        "# Install PyTorch\n",
        "!{pytorch_cmd}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUtKvRnvkaBs"
      },
      "source": [
        "## 4. Clone WanGP Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "emi81EH4kaBt",
        "outputId": "3d950d11-0533-4da7-b8a1-00457547ef82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning repository from https://github.com/remphanostar/WanBook\n",
            "Cloning into 'WanBook'...\n",
            "remote: Enumerating objects: 274, done.\u001b[K\n",
            "remote: Counting objects: 100% (274/274), done.\u001b[K\n",
            "remote: Compressing objects: 100% (253/253), done.\u001b[K\n",
            "remote: Total 274 (delta 22), reused 242 (delta 10), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (274/274), 513.42 KiB | 11.94 MiB/s, done.\n",
            "Resolving deltas: 100% (22/22), done.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Wan2GP'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-860831595.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Change to the repository directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Current directory: {os.getcwd()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Wan2GP'"
          ]
        }
      ],
      "source": [
        "# Clone the repository\n",
        "repo_url = \"https://github.com/remphanostar/WanBook\"\n",
        "repo_dir = \"WanBook\"  # This is now correct\n",
        "\n",
        "if os.path.exists(repo_dir):\n",
        "    print(f\"Repository {repo_dir} already exists. Updating...\")\n",
        "    !cd {repo_dir} && git pull\n",
        "else:\n",
        "    print(f\"Cloning repository from {repo_url}\")\n",
        "    !git clone {repo_url}\n",
        "\n",
        "# Change to the repository directory\n",
        "os.chdir(repo_dir)\n",
        "print(f\"Current directory: {os.getcwd()}\")\n",
        "\n",
        "# List contents to verify\n",
        "!ls -la"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFnmkB7TkaBt"
      },
      "source": [
        "## 5. Install Python Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iIgHhxzkaBt"
      },
      "outputs": [],
      "source": [
        "# Install core requirements\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Install performance optimizations\n",
        "print(\"\\nInstalling performance optimizations...\")\n",
        "\n",
        "# Try to install Triton (for Sage attention)\n",
        "try:\n",
        "    if platform.system() == 'Windows':\n",
        "        !pip install triton-windows\n",
        "    else:\n",
        "        !pip install triton\n",
        "    print(\"✅ Triton installed successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Triton installation failed: {e}\")\n",
        "\n",
        "# Try to install SageAttention\n",
        "try:\n",
        "    !pip install sageattention==1.0.6\n",
        "    print(\"✅ SageAttention installed successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ SageAttention installation failed: {e}\")\n",
        "\n",
        "# Try to install Flash Attention\n",
        "try:\n",
        "    !pip install flash-attn==2.7.2.post1\n",
        "    print(\"✅ Flash Attention installed successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Flash Attention installation failed: {e}\")\n",
        "\n",
        "print(\"\\nDependency installation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdDCeFhQkaBv"
      },
      "source": [
        "## 6. Configuration and Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6oNtXKnkaBv"
      },
      "outputs": [],
      "source": [
        "# Determine optimal settings based on GPU memory - FIXED VERSION\n",
        "def get_optimal_settings():\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "    gpu_name = torch.cuda.get_device_name(0).lower()\n",
        "\n",
        "    settings = {\n",
        "        'model': '--t2v-1-3B',  # Default to smaller model\n",
        "        'profile': '4',  # Just the value, not the flag\n",
        "        'attention': 'sdpa',  # Just the value, not the flag\n",
        "        'extra_flags': []  # Will be processed correctly\n",
        "    }\n",
        "\n",
        "    # GPU-specific optimizations\n",
        "    if gpu_memory >= 24:\n",
        "        settings['model'] = '--t2v-14B'  # Use larger model\n",
        "        settings['profile'] = '3'  # High performance\n",
        "        settings['extra_flags'].extend(['--preload', '2000'])\n",
        "    elif gpu_memory >= 12:\n",
        "        settings['model'] = '--t2v-14B'  # Can handle 14B model\n",
        "        settings['extra_flags'].extend(['--preload', '1000'])\n",
        "    elif gpu_memory >= 8:\n",
        "        settings['extra_flags'].extend(['--preload', '500'])\n",
        "    else:\n",
        "        settings['extra_flags'].extend(['--preload', '0'])\n",
        "        settings['extra_flags'].append('--fp16')\n",
        "\n",
        "    # Try better attention if available\n",
        "    try:\n",
        "        import sageattention\n",
        "        settings['attention'] = 'sage'\n",
        "        print(\"✅ Using Sage Attention\")\n",
        "    except ImportError:\n",
        "        print(\"ℹ️ Using default SDPA attention\")\n",
        "\n",
        "    # Enable compilation if Triton is available\n",
        "    try:\n",
        "        import triton\n",
        "        settings['extra_flags'].append('--compile')\n",
        "        print(\"✅ Compilation enabled\")\n",
        "    except ImportError:\n",
        "        print(\"ℹ️ Compilation disabled (Triton not available)\")\n",
        "\n",
        "    # GPU generation specific settings\n",
        "    if 'rtx 50' in gpu_name:\n",
        "        settings['extra_flags'].append('--fp16')\n",
        "    elif any(x in gpu_name for x in ['rtx 30', 'rtx 40', 'a100', 'v100']):\n",
        "        settings['extra_flags'].extend(['--teacache', '2.0'])\n",
        "    elif any(x in gpu_name for x in ['rtx 20', 'rtx 10', 't4']):\n",
        "        settings['extra_flags'].extend(['--teacache', '1.5'])\n",
        "\n",
        "    return settings\n",
        "\n",
        "optimal_settings = get_optimal_settings()\n",
        "print(f\"Optimal settings for your GPU:\")\n",
        "for key, value in optimal_settings.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjKCaomwkaBv"
      },
      "source": [
        "## 7. Create Launch Scripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBPBZ10ykaBv"
      },
      "outputs": [],
      "source": [
        "# Create launch script\n",
        "def create_launch_script(settings, filename=\"launch_wan2gp.py\"):\n",
        "    script_content = f'''#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "WanGP Auto-Launch Script for Cloud GPU Platforms\n",
        "Generated automatically with optimal settings for your hardware\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import time\n",
        "import signal\n",
        "\n",
        "def launch_wan2gp():\n",
        "    # Base command\n",
        "    cmd = [\n",
        "        sys.executable, \"wgp.py\",\n",
        "        \"{settings['model']}\",\n",
        "        \"{settings['profile']}\",\n",
        "        \"{settings['attention']}\",\n",
        "        \"--listen\",  # Allow external connections\n",
        "        \"--server-port\", \"7860\",\n",
        "        \"--share\",  # Create shareable link\n",
        "    ]\n",
        "\n",
        "    # Add extra flags\n",
        "    extra_flags = {settings['extra_flags']}\n",
        "    for flag in extra_flags:\n",
        "        if flag:\n",
        "            cmd.extend(flag.split())\n",
        "\n",
        "    print(\"Starting WanGP with command:\")\n",
        "    print(\" \".join(cmd))\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"WanGP is starting up...\")\n",
        "    print(\"This may take a few minutes for first-time model downloads.\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    # Launch the process\n",
        "    try:\n",
        "        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
        "                                 universal_newlines=True, bufsize=1)\n",
        "\n",
        "        # Monitor output\n",
        "        for line in process.stdout:\n",
        "            print(line.rstrip())\n",
        "            if \"Running on\" in line:\n",
        "                print(\"\\n\" + \"=\"*50)\n",
        "                print(\"🎉 WanGP is ready!\")\n",
        "                print(\"Access the web interface at the URLs shown above.\")\n",
        "                print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nShutting down WanGP...\")\n",
        "        process.terminate()\n",
        "        process.wait()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    launch_wan2gp()\n",
        "'''\n",
        "\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(script_content)\n",
        "\n",
        "    # Make it executable\n",
        "    os.chmod(filename, 0o755)\n",
        "    print(f\"Created launch script: {filename}\")\n",
        "\n",
        "create_launch_script(optimal_settings)\n",
        "\n",
        "# Create quick launch commands file\n",
        "quick_commands = f'''\n",
        "# WanGP Quick Launch Commands\n",
        "\n",
        "## Optimized for your GPU:\n",
        "python wgp.py {optimal_settings['model']} {optimal_settings['profile']} {optimal_settings['attention']} --listen --share {' '.join(optimal_settings['extra_flags'])}\n",
        "\n",
        "## Alternative modes:\n",
        "\n",
        "# Text-to-Video (default)\n",
        "python wgp.py --t2v --listen --share\n",
        "\n",
        "# Image-to-Video\n",
        "python wgp.py --i2v --listen --share\n",
        "\n",
        "# VACE ControlNet (advanced)\n",
        "python wgp.py --vace-1-3B --listen --share\n",
        "\n",
        "# Low VRAM mode\n",
        "python wgp.py --t2v-1-3B --profile 4 --attention sdpa --fp16 --listen --share\n",
        "\n",
        "# High performance mode (24GB+ VRAM)\n",
        "python wgp.py --t2v-14B --profile 3 --compile --attention sage2 --teacache 2.0 --listen --share\n",
        "\n",
        "'''\n",
        "\n",
        "with open('quick_commands.txt', 'w') as f:\n",
        "    f.write(quick_commands)\n",
        "\n",
        "print(\"Created quick_commands.txt with various launch options\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do6e4nkqkaBw"
      },
      "source": [
        "## 8. Environment-Specific Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saVVE93lkaBw"
      },
      "outputs": [],
      "source": [
        "# Environment-specific configurations\n",
        "def setup_environment_specific():\n",
        "    if env == 'colab':\n",
        "        print(\"Setting up for Google Colab...\")\n",
        "        # Enable GPU persistence\n",
        "        !nvidia-smi -pm 1\n",
        "\n",
        "        # Create ngrok tunnel setup\n",
        "        colab_setup = '''\n",
        "# Google Colab specific setup\n",
        "# Install ngrok for stable URL (optional)\n",
        "!pip install pyngrok\n",
        "\n",
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(7860)\n",
        "print(f\"WanGP will be available at: {public_url}\")\n",
        "'''\n",
        "        with open('colab_setup.py', 'w') as f:\n",
        "            f.write(colab_setup)\n",
        "\n",
        "    elif env == 'kaggle':\n",
        "        print(\"Setting up for Kaggle...\")\n",
        "        # Kaggle has internet disabled by default in some modes\n",
        "        print(\"⚠️ Ensure internet access is enabled in Kaggle settings\")\n",
        "\n",
        "    elif env == 'paperspace':\n",
        "        print(\"Setting up for Paperspace Gradient...\")\n",
        "        # Paperspace specific optimizations\n",
        "        !nvidia-smi -pm 1\n",
        "\n",
        "    elif env == 'runpod':\n",
        "        print(\"Setting up for RunPod...\")\n",
        "        # RunPod usually has good GPU settings by default\n",
        "        pass\n",
        "\n",
        "    else:\n",
        "        print(\"Generic cloud setup applied\")\n",
        "\n",
        "setup_environment_specific()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4LysjrDkaBw"
      },
      "source": [
        "## 9. Model Download and Verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b4U4FyZkaBx"
      },
      "outputs": [],
      "source": [
        "# Test installation and trigger model download\n",
        "print(\"Testing WanGP installation...\")\n",
        "print(\"This will download the required models (may take several minutes)\")\n",
        "\n",
        "# Create a test script that runs briefly\n",
        "test_script = '''\n",
        "import sys\n",
        "import torch\n",
        "import time\n",
        "\n",
        "print(\"Starting WanGP test...\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "\n",
        "# Import WanGP modules to verify installation\n",
        "try:\n",
        "    sys.path.append('.')\n",
        "    # This will trigger model downloads\n",
        "    print(\"Testing model loading...\")\n",
        "    print(\"✅ WanGP installation verified!\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error: {e}\")\n",
        "'''\n",
        "\n",
        "with open('test_installation.py', 'w') as f:\n",
        "    f.write(test_script)\n",
        "\n",
        "# Run the test\n",
        "!python test_installation.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On7l8IcwkaBx"
      },
      "source": [
        "## 10. Launch WanGP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXL76NeckaBx"
      },
      "outputs": [],
      "source": [
        "# Properly build launch command with correct environment variable handling\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def build_launch_command(settings):\n",
        "    cmd = [sys.executable, \"wgp.py\"]\n",
        "\n",
        "    # Add model flag\n",
        "    cmd.append(settings['model'])\n",
        "\n",
        "    # Add profile and attention flags with proper separation\n",
        "    cmd.extend([\"--profile\", settings['profile']])\n",
        "    cmd.extend([\"--attention\", settings['attention']])\n",
        "\n",
        "    # Add fixed server options\n",
        "    cmd.extend([\"--listen\", \"--server-port\", \"7860\", \"--share\"])\n",
        "\n",
        "    # Process extra flags (ensuring complete separation of flags and values)\n",
        "    i = 0\n",
        "    extra_flags = settings['extra_flags']\n",
        "    while i < len(extra_flags):\n",
        "        flag = extra_flags[i]\n",
        "        if flag.startswith('--') and i + 1 < len(extra_flags) and not extra_flags[i + 1].startswith('--'):\n",
        "            cmd.extend([flag, extra_flags[i + 1]])\n",
        "            i += 2\n",
        "        else:\n",
        "            cmd.append(flag)\n",
        "            i += 1\n",
        "\n",
        "    return cmd\n",
        "\n",
        "launch_cmd = build_launch_command(optimal_settings)\n",
        "print(\"🚀 Launching WanGP with corrected settings...\")\n",
        "print(\"Command:\", \" \".join(launch_cmd))\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"📝 IMPORTANT NOTES:\")\n",
        "print(\"• First launch will download models (this may take 10-20 minutes)\")\n",
        "print(\"• The web interface will be available at the URLs shown below\")\n",
        "print(\"• Use Ctrl+C to stop the server\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Execute the launch command\n",
        "try:\n",
        "    process = subprocess.Popen(launch_cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True, bufsize=1)\n",
        "    url_found = False\n",
        "    model_loaded = False\n",
        "\n",
        "    for line in process.stdout:\n",
        "        print(line.rstrip())\n",
        "\n",
        "        if \"Running on\" in line and not url_found:\n",
        "            print(\"\\n\" + \"🎉 \"*5)\n",
        "            print(\"✅ WanGP is now running!\")\n",
        "            print(\"Access the web interface at the URL above\")\n",
        "            print(\"🎉 \"*5 + \"\\n\")\n",
        "            url_found = True\n",
        "\n",
        "        elif \"Model loaded\" in line and not model_loaded:\n",
        "            print(\"\\n📦 Model loaded successfully!\\n\")\n",
        "            model_loaded = True\n",
        "\n",
        "        elif \"Out of memory\" in line or \"CUDA out of memory\" in line:\n",
        "            print(\"\\n⚠️ GPU OUT OF MEMORY ERROR DETECTED\")\n",
        "            print(\"💡 Try launching with a smaller model (e.g., --t2v-1-3B --fp16)\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n🛑 Shutting down WanGP...\")\n",
        "    process.terminate()\n",
        "    process.wait()\n",
        "    print(\"✅ WanGP stopped successfully\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Error launching WanGP: {e}\")\n",
        "    print(\"💡 Try a minimal launch: python wgp.py --listen --share\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klGCmBuLkaBx"
      },
      "source": [
        "## 11. Usage Instructions and Tips"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdEwbHuHkaBx"
      },
      "source": [
        "### 🎯 Quick Start Guide\n",
        "\n",
        "1. **Access the Interface**: Open the web URL shown above\n",
        "2. **Select Model**: Choose from the dropdown (1.3B for speed, 14B for quality)\n",
        "3. **Enter Prompt**: Describe what video you want to generate\n",
        "4. **Click Generate**: Wait for your video to be created\n",
        "\n",
        "### 🎨 Generation Tips\n",
        "\n",
        "**Text-to-Video Prompts:**\n",
        "- \"A cat walking through a sunny garden\"\n",
        "- \"Cinematic shot of waves crashing on rocks at sunset\"\n",
        "- \"A person dancing in the rain, slow motion\"\n",
        "\n",
        "**For Best Results:**\n",
        "- Be descriptive but concise\n",
        "- Include camera angles (\"close-up\", \"wide shot\", \"cinematic\")\n",
        "- Mention lighting (\"golden hour\", \"soft lighting\", \"dramatic\")\n",
        "- Specify motion (\"slow motion\", \"fast paced\", \"gentle movement\")\n",
        "\n",
        "### ⚙️ Advanced Features\n",
        "\n",
        "**Image-to-Video:**\n",
        "1. Switch to Image-to-Video mode\n",
        "2. Upload an image\n",
        "3. Add a text prompt describing the desired motion\n",
        "\n",
        "**VACE ControlNet:**\n",
        "- Motion transfer from reference videos\n",
        "- Object/person injection into scenes\n",
        "- Video inpainting and outpainting\n",
        "- Advanced video manipulation\n",
        "\n",
        "**LoRA Customization:**\n",
        "- Add custom styles and characters\n",
        "- Mix multiple LoRAs for unique effects\n",
        "- Adjust strength with multipliers\n",
        "\n",
        "### 🚨 Troubleshooting\n",
        "\n",
        "**Out of Memory Errors:**\n",
        "- Use smaller models (1.3B instead of 14B)\n",
        "- Reduce frame count (shorter videos)\n",
        "- Enable `--fp16` flag\n",
        "- Use `--profile 4` for memory efficiency\n",
        "\n",
        "**Slow Generation:**\n",
        "- Enable compilation with `--compile`\n",
        "- Use faster attention: `--attention sage`\n",
        "- Enable TeaCache: `--teacache 2.0`\n",
        "\n",
        "**Model Download Issues:**\n",
        "- Ensure stable internet connection\n",
        "- Check available disk space (models are several GB)\n",
        "- Restart if downloads are interrupted\n",
        "\n",
        "### 📊 Performance Optimization\n",
        "\n",
        "**GPU Memory Usage:**\n",
        "- 1.3B models: ~6GB VRAM\n",
        "- 14B models: ~12-24GB VRAM\n",
        "- VACE models: Similar to base models\n",
        "\n",
        "**Speed vs Quality Trade-offs:**\n",
        "- Fewer steps = faster generation, lower quality\n",
        "- More steps = slower generation, higher quality\n",
        "- TeaCache = 2x speed boost with minimal quality loss\n",
        "\n",
        "### 🔗 Useful Commands\n",
        "\n",
        "Check the `quick_commands.txt` file for various launch options tailored to different use cases and hardware configurations."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}